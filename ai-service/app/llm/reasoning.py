import json
import re
from .llm_client import client  # client = OpenAI(api_key=...)
import os
import json
import pandas as pd

def detect_claim_category(claim: str) -> str:
    base_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(base_dir, "category.csv")

    print("üìå ƒêang load Category CSV t·ª´:", csv_path)

    # Check t·ªìn t·∫°i file
    if not os.path.exists(csv_path):
        print("‚ùå Kh√¥ng t√¨m th·∫•y category.csv")
        return None
    # Load CSV
    df = pd.read_csv(csv_path)
    categories = df['category'].dropna().str.strip().unique().tolist()

    categories_text = ", ".join(categories)

    prompt = f"""
    B·∫°n l√† h·ªá th·ªëng ph√¢n lo·∫°i tin t·ª©c theo lƒ©nh v·ª±c.

    Claim: "{claim}"

    C√°c lƒ©nh v·ª±c hi·ªán c√≥:
    {categories_text}

    Ch·ªâ tr·∫£ v·ªÅ duy nh·∫•t t√™n lƒ©nh v·ª±c ph√π h·ª£p nh·∫•t.
    """

    completion = client.chat.completions.create(
        model="GPT‚Äë4o mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return completion.choices[0].message.content.strip()


def llm_filter_evidence(claim, triples, claim_category):
    print("\n=== FILTER BY CATEGORY STEP ===")
    print("Category detected for claim:", claim_category)

    # 1Ô∏è‚É£ L·ªçc triples theo category ph√π h·ª£p tr∆∞·ªõc khi g·ªçi LLM
    category_filtered = []
    for t in triples:
        cat = t.get("category", "")
        if claim_category in cat:
            category_filtered.append(t)

    print("\nTriples after category filtering:")
    print(json.dumps(category_filtered, indent=2, ensure_ascii=False))

    # N·∫øu l·ªçc h·∫øt ‚Üí fallback gi·ªØ t·∫•t c·∫£ triples
    if not category_filtered:
        print("\n‚ö† No category matched ‚Äî fallback to all triples")
        category_filtered = triples

    # 2Ô∏è‚É£ Ch·ªâ g·ª≠i evidence text sang LLM
    evidence_sentences = [
        t.get("evidence", "") for t in category_filtered if t.get("evidence")
    ]

    print("\nEvidence sent to LLM:")
    print(json.dumps(evidence_sentences, indent=2, ensure_ascii=False))

    # Prompt l·ªçc theo n·ªôi dung
    prompt = f"""
    Claim:
    "{claim}"

    D∆∞·ªõi ƒë√¢y l√† c√°c c√¢u evidence ƒë√£ qua b∆∞·ªõc l·ªçc theo category:
    {json.dumps(evidence_sentences, ensure_ascii=False)}

    Nhi·ªám v·ª•:
    - Ch·ªâ gi·ªØ l·∫°i nh·ªØng c√¢u li√™n quan tr·ª±c ti·∫øp ƒë·ªÉ ki·ªÉm ch·ª©ng claim
    - Tr·∫£ v·ªÅ JSON LIST duy nh·∫•t g·ªìm c√°c c√¢u evidence ph√π h·ª£p
    """

    completion = client.chat.completions.create(
        model="GPT‚Äë4o mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    resp = completion.choices[0].message.content.strip()
    cleaned = re.sub(r"```json|```", "", resp).strip()

    try:
        selected_sentences = json.loads(cleaned)
    except:
        print("\n‚ö† Parse failed ‚Äî fallback to category-filtered triples")
        return category_filtered

    # Map sentence ‚Üí triple
    final = []
    for s in selected_sentences:
        for t in category_filtered:
            if s == t.get("evidence"):
                final.append(t)
                break

    print("\n=== FINAL TRIPLES SELECTED:")
    print(json.dumps(final, indent=2, ensure_ascii=False))

    return final





def llm_reasoning(claim: str, evidence: str):
    prompt = f"""
    B·∫°n l√† tr·ª£ l√Ω ki·ªÉm ch·ª©ng th√¥ng tin d·ª±a tr√™n tri th·ª©c c√≥ c·∫•u tr√∫c t·ª´ Knowledge Graph.

    Nhi·ªám v·ª• c·ªßa b·∫°n:

    1. ∆Øu ti√™n x√°c ƒë·ªãnh claim l√† TRUE ho·∫∑c FALSE tr∆∞·ªõc.
    - N·∫øu Knowledge Graph ch·ª©a th√¥ng tin h·ªó tr·ª£ claim ‚Üí tr·∫£ l·ªùi TRUE.
    - N·∫øu Knowledge Graph c√≥ th√¥ng tin m√¢u thu·∫´n, ph·ªß ƒë·ªãnh claim ‚Üí tr·∫£ l·ªùi FALSE.

    2. Ch·ªâ tr·∫£ l·ªùi NEI khi th·∫≠t s·ª± kh√¥ng c√≥ b·∫•t k·ª≥ d·ªØ ki·ªán n√†o li√™n quan ƒë·∫øn claim,
    ho·∫∑c c√°c d·ªØ ki·ªán kh√¥ng ƒë·ªß ƒë·ªÉ k·∫øt lu·∫≠n r√µ r√†ng.

    3. Gi·∫£i th√≠ch b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n v√† thuy·∫øt ph·ª•c,
    d·ª±a v√†o hi·ªÉu bi·∫øt t·ª´ c√°c facts trong Knowledge Graph,
    nh∆∞ng kh√¥ng ƒë∆∞·ª£c tr√≠ch nguy√™n vƒÉn entity ho·∫∑c quan h·ªá.
    H√£y chuy·ªÉn ch√∫ng th√†nh c√¢u m√¥ t·∫£ t·ª± nhi√™n.

    D·ªØ li·ªáu:

    CLAIM: "{claim}"

    FACTS T·ª™ KNOWLEDGE GRAPH:
    {evidence}

    H√£y tr·∫£ v·ªÅ JSON v·ªõi c·∫•u tr√∫c:
    {{
        "verdict": "TRUE | FALSE | NEI",
        "explanation": ""L·ªùi gi·∫£i th√≠ch ph·∫£i t·ª± nhi√™n nh∆∞ ng∆∞·ªùi th·∫≠t, ch·ªâ t·∫≠p trung v√†o n·ªôi dung c·ªßa claim v√† th√¥ng tin ƒë√£ cho. 
Kh√¥ng nh·∫Øc ƒë·∫øn 'Knowledge Graph', 'b·∫±ng ch·ª©ng', 'd·ªØ li·ªáu', 'ngu·ªìn', 'h·ªá th·ªëng', 
hay b·∫•t k·ª≥ y·∫øu t·ªë k·ªπ thu·∫≠t n√†o. 
Ch·ªâ m√¥ t·∫£ l·∫°i s·ª± ki·ªán m·ªôt c√°ch m·∫°ch l·∫°c, d·ªÖ hi·ªÉu v√† logic nh∆∞ ƒëang t∆∞·ªùng thu·∫≠t l·∫°i n·ªôi dung."
"
    }}
    """

    completion = client.chat.completions.create(
    model="GPT‚Äë4o mini",
    messages=[
        {"role": "user", "content": prompt}
    ]
)

    resp_text = completion.choices[0].message.content

    cleaned = re.sub(r"```json|```", "", resp_text).strip()
    return json.loads(cleaned)

